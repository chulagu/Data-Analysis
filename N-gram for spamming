
import json
from collections import Counter, defaultdict
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
import nltk
import re

# Download required NLTK data
nltk.download('punkt')

# Function to clean and tokenize text
def preprocess_text(text):
    text = re.sub(r'#\S+', '', text)  # Remove words coming after #
    tokens = word_tokenize(text.lower())
    return [token for token in tokens if token.isalnum() and token != "https"]

# Function to get tweet's original URL
def get_original_url(entry):
    base_url = "https://twitter.com/"
    username = entry['author']['username']
    tweet_id = entry['id']
    return f"{base_url}{username}/status/{tweet_id}"

# Load and process the dataset
dataset_file = 'xxx.json'

with open(dataset_file, 'r', encoding='utf-8') as file:
    data = json.load(file)

# Ensure unique entries
unique_data = list({entry['id']: entry for entry in data}.values())

# Extract text from dataset and clean/tokenize
texts = [preprocess_text(entry['text']) for entry in unique_data]

# Function to extract n-grams from text
def extract_ngrams(text, n):
    return list(ngrams(text, n))

# Initialize variables
all_ngrams = {}
tagged_tweets = set()
ngram_level_counts = {n: 0 for n in range(6, 1, -1)}

# Loop from 6-grams to 2-grams
for n in range(6, 1, -1):
    ngram_counts = Counter()
    ngram_examples = defaultdict(list)

    # Extract n-grams and count frequencies
    for idx, text in enumerate(texts):
        if idx not in tagged_tweets:
            ngrams_list = extract_ngrams(text, n)
            ngram_counts.update(ngrams_list)
            for ng in ngrams_list:
                ngram_examples[ng].append(idx)

    # Select top n-grams and tag tweets
    for ngram, count in ngram_counts.items():
        if count > 1:  # Only consider n-grams that appear in more than one tweet
            unique_tweet_indices = set(ngram_examples[ngram])
            newly_tagged = unique_tweet_indices - tagged_tweets
            if newly_tagged:
                all_ngrams[(ngram, n)] = [len(newly_tagged), [get_original_url(unique_data[i]) for i in newly_tagged][:4]]
                tagged_tweets.update(newly_tagged)
                ngram_level_counts[n] += len(newly_tagged)

# Sort all_ngrams by count in descending order
sorted_ngrams = sorted(all_ngrams.items(), key=lambda x: x[1][0], reverse=True)

# Output file path for n-gram analysis
output_ngram_file = 'ngram_analysis.txt'

# Write n-gram analysis results to a text file
with open(output_ngram_file, 'w', encoding='utf-8') as out_file:
    total_unique_count = sum(ngram_level_counts.values())
    out_file.write(f"Total Unique Count of All N-grams: {total_unique_count}\n\n")
    for n in range(6, 1, -1):
        out_file.write(f"Total Tweets Tagged at {n}-gram Level: {ngram_level_counts[n]}\n")
    out_file.write("\n")

    for (ngram, n), (count, urls) in sorted_ngrams:
        ngram_str = ' '.join(ngram)
        out_file.write(f"{n}-gram: {ngram_str} (Count: {count})\n")
        for url in urls:
            out_file.write(f"- {url}\n")
        out_file.write("\n")
    print(f"N-gram analysis results saved to {output_ngram_file}")
